{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt   \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import pylab as pl\n",
    "from matplotlib.pyplot import figure\n",
    "from collections import Counter\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth_file = pd.read_csv('/Users/sleung2/Documents/MIDS Program/Capstone_local/snapshot_wisconsin/all/yolo_splits3/test/test_labels.csv')\n",
    "truth_file = truth_file.rename(columns = {'TRIGGER_ID': 'event_id',\n",
    "                                               'CLASS_SPECIES': 'true_class',\n",
    "                                               'Total': 'true_count',\n",
    "                                               'CLASS_SPECIES_RESTATED': 'true_class_name'})\n",
    "\n",
    "truth_file_no_other = truth_file[truth_file['true_class_name'] != 'other']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.DataFrame(['foxgray_foxred',\n",
    "              'cottontail_snowshoehare',\n",
    "              'raccoon',\n",
    "              'opossum',\n",
    "              'turkey',\n",
    "              'bear',\n",
    "              'elk',\n",
    "              'deer',\n",
    "              'coyote',\n",
    "              'wolf']).sort_values(0)\n",
    "labels = labels.rename(columns = {0: 'species'})\n",
    "labels.insert(0, 'class', range(0, len(labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2- Species"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read lines from txt results file\n",
    "\n",
    "toppath = 'phase 2-species/'\n",
    "txt_file = 'output_no_wolf.txt'\n",
    "\n",
    "with open(toppath + txt_file, 'r') as f:\n",
    "    original_lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Txt file to dataframe\n",
    "#We are taking the TOP confidence score species for each image as the species\n",
    "#Count is the count for all bounding boxes drawn in image\n",
    "\n",
    "rows = []\n",
    "\n",
    "for item in original_lines:\n",
    "    \n",
    "    #Class number\n",
    "    item_split_1 = item.split(' Bbox[list]')[0]\n",
    "    class_conf_list = item_split_1.split(';')[1:-2]\n",
    "    \n",
    "    conf_score_max = 0\n",
    "    \n",
    "    i = 0    \n",
    "    for class_conf in class_conf_list:\n",
    "        conf_score_int = float(class_conf.split(',Conf:')[1])\n",
    "        \n",
    "        \n",
    "        if conf_score_int > conf_score_max:\n",
    "            conf_score_max = conf_score_int\n",
    "            max_index = i\n",
    "            \n",
    "        i += 1\n",
    "    try:\n",
    "        class_string = class_conf_list[max_index].split(',')[0]\n",
    "        class_num = float(class_string.split(':')[1])\n",
    "        class_num = int(class_num)\n",
    "    except:\n",
    "        class_num = 'Blank'\n",
    "\n",
    "    #Counts\n",
    "    count_split_1 = item.split(' Bbox[list]')[-1]\n",
    "    count_split_2 = count_split_1.replace(';', ',')  \n",
    "    count_split_3 = count_split_2.split(',')\n",
    "    count = int((len(count_split_3)-1)/4)\n",
    "        \n",
    "    #Filename\n",
    "    file_split_1 = item.split('Filename: ')[1]\n",
    "    file_name = file_split_1.split(';')[0].split('.jpg')[0]\n",
    "        \n",
    "    row = [file_name, class_num, count, conf_score_max]\n",
    "    \n",
    "    rows.append(row)       \n",
    "\n",
    "predictions = pd.DataFrame(rows, columns = ['filename', 'class', 'count', 'conf_score_max'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Species\n",
    "1. Events with multiple images of same class will be labeled the majority class\n",
    "- For event to be labeled Blank, all images must be blank\n",
    "2. Events with all different labels will get labelled with highest confidence score \n",
    "\n",
    "Counts\n",
    "1. Events with multiple images of same class- we will take the max of the majority class\n",
    "2. Events with all different labels will get labelled with the count of highest conf score label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Event column- remove A,B,C appendix\n",
    "predictions['event_id'] = predictions['filename'].str[:-1]\n",
    "\n",
    "\n",
    "## Events with muliple images of same class\n",
    "#Species determination\n",
    "#Groupby event, class\n",
    "event_class_group_count = pd.DataFrame(predictions.groupby(['event_id', 'class']).count()['filename']).reset_index().rename(columns = {'filename': 'count_rows'})\n",
    "\n",
    "#Filter by rows that have count greater than 1\n",
    "event_class_group_count_majority = event_class_group_count[event_class_group_count['count_rows'] > 1]\n",
    "\n",
    "#Remove blank events that are not a consensus for all 3 images\n",
    "event_class_group_count_majority = event_class_group_count_majority[~((event_class_group_count_majority['class'] == 'Blank') &\n",
    "                                (event_class_group_count_majority['count_rows'] < 3))]\n",
    "\n",
    "#Count determination\n",
    "event_class_group_majority_counts = pd.merge(predictions, event_class_group_count_majority,\n",
    "         how = 'inner',\n",
    "         on = ['event_id', 'class'])\n",
    "\n",
    "event_class_majority_counts = event_class_group_majority_counts[['event_id', 'count']].groupby(['event_id']).max().reset_index()\n",
    "\n",
    "event_max_conf_score = event_class_group_majority_counts[['event_id', 'conf_score_max']].groupby(['event_id']).max().reset_index()\n",
    "\n",
    "majority_df = pd.merge(event_class_majority_counts, event_class_group_count_majority,\n",
    "         on = 'event_id',\n",
    "         how = 'left')[['event_id', 'count','class']]\n",
    "\n",
    "majority_df = pd.merge(majority_df, event_max_conf_score,\n",
    "         on = 'event_id',\n",
    "         how = 'left')\n",
    "\n",
    "## Events with all different labels\n",
    "predictions_single_count = predictions[~predictions['event_id'].isin(majority_df['event_id'])]\n",
    "\n",
    "event_list = []\n",
    "pred_list = []\n",
    "count_list = []\n",
    "conf_score_max_list = []\n",
    "\n",
    "previous_event = ''\n",
    "conf_score_group_max = 0\n",
    "pred_class = ''\n",
    "conf_score_group_max = 0\n",
    "\n",
    "predictions_single_count_grouped = predictions_single_count.groupby(['event_id', 'class',\n",
    "                                                                     'conf_score_max'])\n",
    "\n",
    "for group_name, group in predictions_single_count_grouped:\n",
    "        \n",
    "    current_event = group_name[0]\n",
    "    current_class = group['class'].iloc[0]\n",
    "    current_conf_score = group['conf_score_max'].iloc[0]\n",
    "    current_count = group['count'].iloc[0]\n",
    "    \n",
    "    #Check if we are looking at a new event\n",
    "    if current_event != previous_event:\n",
    "        conf_score_max_list.append(conf_score_group_max)\n",
    "        conf_score_group_max = 0\n",
    "        event_list.append(previous_event)\n",
    "        pred_list.append(pred_class)\n",
    "        count_list.append(current_count)\n",
    "        \n",
    "    if conf_score_group_max < current_conf_score:\n",
    "        pred_class = current_class\n",
    "        conf_score_group_max = current_conf_score\n",
    "        \n",
    "    previous_event = current_event \n",
    "\n",
    "conf_score_max_list.append(conf_score_group_max)\n",
    "event_list.append(previous_event)\n",
    "pred_list.append(pred_class)\n",
    "count_list.append(current_count)\n",
    "    \n",
    "single_class_df = pd.DataFrame(list(zip(event_list,\n",
    "              pred_list,\n",
    "              count_list,\n",
    "              conf_score_max_list)),\n",
    "            columns = ['event_id', 'class', 'count','conf_score_max'])\n",
    "\n",
    "predictions_by_events_df = pd.concat([majority_df, single_class_df])\n",
    "\n",
    "\n",
    "yolo_predictions_by_events_df = pd.merge(predictions_by_events_df, labels,\n",
    "         how = 'left',\n",
    "         on = 'class')\n",
    "\n",
    "truth_pred_yolo_df = pd.merge(truth_file_no_other, yolo_predictions_by_events_df,\n",
    "                         how = 'left',\n",
    "                         left_on = 'event_id',\n",
    "                         right_on = 'event_id')\n",
    "yolo_preds_df = truth_pred_yolo_df[['event_id', 'class', 'conf_score_max', 'count', 'species']]\n",
    "\n",
    "yolo_preds_df = yolo_preds_df.rename(columns = {'class': 'pred_class',\n",
    "                                               'conf_score_max': 'conf_score',\n",
    "                                                'count': 'pred_count',\n",
    "                                               'species': 'pred_class_name'})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EffNet, Megadetecter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = pd.read_json('phase 2-species/phase2_efficientnetb5_classifications.json')\n",
    "df = model_results['phase2_classification_results'].apply(pd.Series)\n",
    "df['event_id'] = df['id'].str.split('.jpg').str[0]\n",
    "df['event_id'] = df['event_id'].str[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for index, row in df.iloc[:5].iterrows():\n",
    "preds_dict = {}\n",
    "for index, row in df.iterrows():\n",
    "  event_id = str(row['event_id'])\n",
    "  pred_class = row['class']\n",
    "  pred_conf = row['conf']\n",
    "  #print(event_id, pred_class, pred_conf)\n",
    "  \n",
    "  result_dict = {\n",
    "        \"class\": pred_class,\n",
    "        \"conf\": pred_conf\n",
    "    }\n",
    "  \n",
    "  if event_id in preds_dict:\n",
    "    preds_dict[event_id].append(result_dict)\n",
    "  else:\n",
    "    preds_dict[event_id] = [result_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "final_preds_dict = {}\n",
    "for key, value in preds_dict.items():\n",
    "  event_id = key\n",
    "  counts = Counter(d['class'] for d in value)\n",
    "\n",
    "  ## if all 3 predictions are different, defer to class with highest confidence\n",
    "  if len(counts) == 3:\n",
    "    highest_conf = max([x['conf'] for x in value])\n",
    "    pred_class = [x['class'] for x in value if x['conf']==highest_conf][0]\n",
    "  \n",
    "  ## if there is an even number of predictions (2), defer to class with higher confidence\n",
    "  elif sum(counts.values()) < 3:\n",
    "    highest_conf = max([x['conf'] for x in value])\n",
    "    pred_class = [x['class'] for x in value if x['conf']==highest_conf][0]\n",
    "\n",
    "  ## otherwise, class is based on majority class, conf score is based on highest score for\n",
    "  ## majority class\n",
    "  else:\n",
    "    most_common = {'most_common': counts.most_common(1)[0][0]}\n",
    "    pred_class = most_common['most_common']\n",
    "    highest_conf = max([x['conf'] for x in value if x['class'] == most_common['most_common']])\n",
    "\n",
    "  final_preds_dict[event_id] = [pred_class, highest_conf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "eff_net_preds_df = pd.DataFrame.from_dict(final_preds_dict,  orient='index').reset_index()\n",
    "eff_net_preds_df.columns=['event_id', 'pred_class', 'conf_score']\n",
    "label_mapping = dict({0:'bear', 1:'cottontail_snowshoehare', 2:'coyote', 3:'deer', 4:'elk', 5:'foxgray_foxred', 6:'opossum', 7:'raccoon', 8:'turkey', 9:'wolf'})\n",
    "eff_net_preds_df['pred_class_name'] = eff_net_preds_df['pred_class'].map(label_mapping)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consolidate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "eff_net_preds_df['model'] = 'eff_net'\n",
    "yolo_preds_df['model'] = 'yolo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_pred_df = pd.concat([eff_net_preds_df, yolo_preds_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_max_conf_lookup = ensemble_pred_df[['event_id', 'conf_score']].groupby(['event_id']).max().reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_max_conf_pred_df =pd.merge(ensemble_pred_df,ensemble_max_conf_lookup,\n",
    "         how = 'inner',\n",
    "         on = ['event_id', 'conf_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_result_metrics = pd.merge(truth_file_no_other, ensemble_max_conf_pred_df,\n",
    "         how = 'left',\n",
    "         on = ['event_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         precision    recall  f1-score   support\n",
      "\n",
      "                   bear       0.68      0.81      0.74       130\n",
      "cottontail_snowshoehare       0.62      0.37      0.46       121\n",
      "                 coyote       0.47      0.33      0.39       135\n",
      "                   deer       0.82      0.34      0.48       191\n",
      "                    elk       0.86      0.48      0.61       123\n",
      "         foxgray_foxred       0.36      0.39      0.37        70\n",
      "                opossum       0.83      0.38      0.53        52\n",
      "                raccoon       0.67      0.41      0.51       119\n",
      "                 turkey       0.47      0.69      0.56        64\n",
      "                   wolf       0.28      0.87      0.42       126\n",
      "\n",
      "               accuracy                           0.50      1131\n",
      "              macro avg       0.61      0.51      0.51      1131\n",
      "           weighted avg       0.62      0.50      0.51      1131\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true = ensemble_result_metrics['true_class_name']\n",
    "y_pred = ensemble_result_metrics['pred_class_name']\n",
    "label_list = np.unique(y_true)\n",
    "\n",
    "target_names = label_list\n",
    "print(classification_report(y_true, y_pred, target_names=target_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         precision    recall  f1-score   support\n",
      "\n",
      "                   bear       0.78      0.82      0.80       109\n",
      "cottontail_snowshoehare       0.42      0.23      0.30        83\n",
      "                 coyote       0.29      0.11      0.16        84\n",
      "                   deer       0.82      0.21      0.33       136\n",
      "                    elk       0.94      0.46      0.62        96\n",
      "         foxgray_foxred       0.22      0.21      0.21        43\n",
      "                opossum       0.76      0.33      0.46        39\n",
      "                raccoon       0.56      0.27      0.37        73\n",
      "                 turkey       0.49      0.67      0.56        55\n",
      "                   wolf       0.28      0.95      0.43       116\n",
      "\n",
      "               accuracy                           0.45       834\n",
      "              macro avg       0.56      0.43      0.42       834\n",
      "           weighted avg       0.58      0.45      0.44       834\n",
      "\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "                   bear       0.39      0.76      0.52        21\n",
      "cottontail_snowshoehare       0.93      0.68      0.79        38\n",
      "                 coyote       0.56      0.71      0.63        51\n",
      "                   deer       0.82      0.67      0.74        55\n",
      "                    elk       0.68      0.56      0.61        27\n",
      "         foxgray_foxred       0.51      0.67      0.58        27\n",
      "                opossum       1.00      0.54      0.70        13\n",
      "                raccoon       0.78      0.63      0.70        46\n",
      "                 turkey       0.39      0.78      0.52         9\n",
      "                   wolf       0.00      0.00      0.00        10\n",
      "\n",
      "               accuracy                           0.64       297\n",
      "              macro avg       0.61      0.60      0.58       297\n",
      "           weighted avg       0.68      0.64      0.64       297\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sleung2/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/sleung2/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/sleung2/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "for model in ['eff_net', 'yolo']:\n",
    "    ensemble_result_metrics_model = ensemble_result_metrics[ensemble_result_metrics['model'] == model]\n",
    "    \n",
    "    y_true = ensemble_result_metrics_model['true_class_name']\n",
    "    y_pred = ensemble_result_metrics_model['pred_class_name']\n",
    "    label_list = np.unique(y_true)\n",
    "\n",
    "    target_names = label_list\n",
    "    print(classification_report(y_true, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "yolo_metrics = pd.merge(yolo_preds_df, truth_file_no_other, \n",
    "         how = 'left',\n",
    "         on = ['event_id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_metrics_not_null = yolo_metrics[~yolo_metrics['pred_class_name'].isnull()]\n",
    "yolo_metrics_not_null = yolo_metrics_not_null[yolo_metrics_not_null['true_class_name'] != 'wolf']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         precision    recall  f1-score   support\n",
      "\n",
      "                   bear       0.63      0.92      0.75       127\n",
      "cottontail_snowshoehare       0.87      0.65      0.75       106\n",
      "                 coyote       0.56      0.65      0.60       131\n",
      "                   deer       0.75      0.56      0.64       176\n",
      "                    elk       0.76      0.50      0.61       115\n",
      "         foxgray_foxred       0.35      0.51      0.42        69\n",
      "                opossum       0.82      0.69      0.75        45\n",
      "                raccoon       0.73      0.66      0.69       106\n",
      "                 turkey       0.64      0.83      0.72        60\n",
      "\n",
      "               accuracy                           0.66       935\n",
      "              macro avg       0.68      0.66      0.66       935\n",
      "           weighted avg       0.69      0.66      0.66       935\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_true = yolo_metrics_not_null['true_class_name']\n",
    "y_pred = yolo_metrics_not_null['pred_class_name']\n",
    "label_list = np.unique(y_true)\n",
    "\n",
    "target_names = label_list\n",
    "print(classification_report(y_true, y_pred, target_names=target_names))\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
